# NLP-Transformer-LLM-fever
Learning NLP
| Day | Topics | Resources |
| --- | ------ | ---------- |
| [Day1](https://github.com/Utshav-paudel/NLP-Transformer-LLM-fever/tree/d04a964a68eac124141649a6ee17b5b366c798bd/LLM_from_scratch/day1) | word,character and subword tokenizer and implemented subword tokenizer |           | 
| [Day2](https://github.com/Utshav-paudel/NLP-Transformer-LLM-fever/tree/d256cc1d35f0e34d5aa1aab6cd51f116dd8287b9/LLM_from_scratch/day2)| Basics of pytorch, processing sequential data in pytorch,Gpu vs cpu performance in pytorch |  |
| [Day3](https://github.com/Utshav-paudel/NLP-Transformer-LLM-fever/tree/d18231ef652e7b22e43ce7f1b3cd869a447da832/LLM_from_scratch/day3)| Pytorch functions and their implementation |  |
| [Day4](https://github.com/Utshav-paudel/NLP-Transformer-LLM-fever/tree/7cd0867079b87b7f3d8aeddb9bd08f6b2d68a5f5/LLM_from_scratch/day4) | Embedding and developed Bigram language model that predict the next toke | |
| [Day5](https://github.com/Utshav-paudel/NLP-Transformer-LLM-fever/tree/dbf245446a69abc335cc61945c38e837aa8e0ecf/LLM_from_scratch/day5) | Created evaluation function for the Bigram language model and optimzation of it | |
| [Day6]() | Started to update Bigram to GPT language model , learned every detail of transformer and created forward pass , weight initialization and decoder portion  | |
| [Day7]() | Created decoder class which contain -> multihead attention -> add and normalize -> feedforward-> add and normalize and also created the feedforward class of decoder that contain  ->Linear ->ReLU -> Linear | |
