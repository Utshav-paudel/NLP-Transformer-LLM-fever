# NLP-Transformer-LLM-fever
Learning NLP and Large Language Model 
| Day | Topics | Resources |
| --- | ------ | ---------- |
| [Day1](https://github.com/Utshav-paudel/NLP-Transformer-LLM-fever/tree/d04a964a68eac124141649a6ee17b5b366c798bd/LLM_from_scratch/day1) | word,character and subword tokenizer and implemented subword tokenizer |           | 
| [Day2](https://github.com/Utshav-paudel/NLP-Transformer-LLM-fever/tree/d256cc1d35f0e34d5aa1aab6cd51f116dd8287b9/LLM_from_scratch/day2)| Basics of pytorch, processing sequential data in pytorch,Gpu vs cpu performance in pytorch |  |
| [Day3](https://github.com/Utshav-paudel/NLP-Transformer-LLM-fever/tree/d18231ef652e7b22e43ce7f1b3cd869a447da832/LLM_from_scratch/day3)| Pytorch functions and their implementation |  |
| [Day4](https://github.com/Utshav-paudel/NLP-Transformer-LLM-fever/tree/7cd0867079b87b7f3d8aeddb9bd08f6b2d68a5f5/LLM_from_scratch/day4) | Embedding and developed Bigram language model that predict the next toke | |
| [Day5](https://github.com/Utshav-paudel/NLP-Transformer-LLM-fever/tree/dbf245446a69abc335cc61945c38e837aa8e0ecf/LLM_from_scratch/day5) | Created evaluation function for the Bigram language model and optimzation of it | |
| [Day6](https://github.com/Utshav-paudel/NLP-Transformer-LLM-fever/tree/47d09f69e3418b0a8932fef92a20c515f9860f2f/LLM_from_scratch/day6) | Started to update Bigram to GPT language model , learned every detail of transformer and created forward pass , weight initialization and decoder portion  | |
| [Day7](https://github.com/Utshav-paudel/NLP-Transformer-LLM-fever/tree/ad114fad69ef9f511dfde64d730d3d7d863bc458/LLM_from_scratch/day7) | Created decoder class which contain -> multihead attention -> add and normalize -> feedforward-> add and normalize and also created the feedforward class of decoder that contain  ->Linear ->ReLU -> Linear | |
|[Day8](https://github.com/Utshav-paudel/NLP-Transformer-LLM-fever/tree/ad114fad69ef9f511dfde64d730d3d7d863bc458/LLM_from_scratch/day8)| Implementation of multihead attention of transformer from sratch which was used in our GPT language model|||
|[Day9](https://github.com/Utshav-paudel/NLP-Transformer-LLM-fever/tree/31cff1117b5a7a3b127f743af06e407a6e251ad1/LLM_from_scratch/day9) |  Implementation of data extracting script that read  large text corpus open source webtext to feed to llm for training| |
|[Day10](https://github.com/Utshav-paudel/NLP-Transformer-LLM-fever/tree/e31c9590855372c01e2759fb5e0b074b76e9f8af/LLM_from_scratch/day10)| Modification on data extracting where extracted data while be divided into 2 part training and validation | |
